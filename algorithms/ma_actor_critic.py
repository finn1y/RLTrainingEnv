#!/usr/bin/env python3

import numpy as np
import tensorflow as tf

from algorithms.actor_critic import ActorCritic

class MAActorCritic():
    """
        Class to contain the ACNetwork and all parameters
    """
    def __init__(self, sizes, gamma=0.99, lr_decay=0.9, lr=0.0001, lr_decay_steps=10000, n_agents=2, master=False, saved_path=None):
        """
            function to initialise the class

            sizes is an array of [observations, hidden_size, actions] where observations is an array of 
            [observations_low, observations_high], hidden_size is the number of neurons in the hidden layer 
            and actions is an array of [actions_low, actions_high] in turn low is an array of low bounds for 
            each observation/action and high is an array of high bounds for each observation/action respectively

            gamma is the discount factor of future rewards

            lr is the learning rate of the neural network

            lr_decay is a float which is the rate at which the learning rate will decay exponentially

            lr_decay_steps is an int which is the number of time steps to decay the learning rate

            n_agents is the number of agents in the environment

            master is a bool to determine if this agent should be a master (contain the global critic net)

            saved_path is a string of the path to the saved Actor-Critic network if one is being loaded
        """
        self.gamma = gamma
        self.lr = lr
        self.lr_decay = lr_decay
        self.n_actions = np.shape(sizes[2])[1]
        self.replay_mem = []
        self.n_agents = n_agents
        self.master = master
        self.eps = np.finfo(np.float32).eps.item()

        self.actor_net = ActorNet(sizes)

        self.lr_decay_fn = tf.keras.optimizers.schedules.ExponentialDecay(self.lr, decay_steps=lr_decay_steps, decay_rate=self.lr_decay)
        self.a_opt = tf.keras.optimizers.Adam(learning_rate=self.lr_decay_fn) #Adam optimiser is...

        if self.master:
            #only master contains the global critic net
            self.critic_net = CriticNet(sizes, self.n_agents)

            self.c_opt = tf.keras.optimizers.Adam(learning_rate=self.lr_decay_fn) #Adam optimiser is...
            self.loss_fn = tf.keras.losses.Huber() #Huber loss is...

        #load a saved model (neural net) if provided
        if saved_path:
            self.actor_net = tf.keras.models.load_model(saved_path, custom_object={"CustomModel": ActorNet})
            self.critic_net = tf.keras.models.load_model(saved_path, custom_object={"CustomModel": CriticNet})

    def get_parameters(self):
        """
            function to get the parameters of the algorithm

            returns a dict with all the algorithm parameters
        """
        return {"gamma": self.gamma, "lr": self.lr, "lr_decay": self.lr_decay}

    def save_model(self, path):
        """
            function to save the tensorflow model (neural net) to a file

            path is a string of the path to the file where the model will be saved
        """
        self.actor_net.save(path)

        if self.master:
            self.critic_net.save(path)

    def get_action(self, obv):
        """
            function to get the action based on the current observation using the 
            policy generated by the neural net

            obv is the current observation of the state

            returns the action to take
        """
        action_probs = self.actor_net(np.array([obv]))
        action = np.random.choice(self.n_actions, p=action_probs.numpy()[0])

        return action

    def store_step(self, obv, action, reward, next_obv):
        """
            function to store an step's tuple of values

            obv is the observation of the current state

            action is an int of the action taken

            reward is the reward returned when the action is applied to the current state

            next obv is the observation of the next state after action has been applied to the current state
        """
        #next_obv is not used in training so doesn't need to be saved
        if self.master:
            self.replay_mem.append({"obvs": [obv], "actions": [action], "rewards": [reward]})
        else:
            self.replay_mem.append({"obv": obv, "action": action, "reward": reward})

    def train(self):
        """
            function to train Actor-Critic network using previous episode data from replay memory

            returns the loss of the training as a tensor
        """
        returns = []
        discounted_sum = 0

        #master agent has a different replay memory structure as it must hold data for all agents not only itself
        if self.master:
            #samples of each piece of data of this agent
            obv_batch = np.array([self.replay_mem[i]["obvs"][0] for i in range(np.shape(self.replay_mem)[0])])
            action_batch = np.array([self.replay_mem[i]["actions"][0] for i in range(np.shape(self.replay_mem)[0])])

            #samples of obvs and actions of all agents flattened for input to critic array
            all_obv_batches = np.array([np.concatenate(self.replay_mem[i]["obvs"], axis=0) for i in range(np.shape(self.replay_mem)[0])])
            all_action_batches = np.array([self.replay_mem[i]["actions"] for i in range(np.shape(self.replay_mem)[0])])

            c_returns = []
            avg_discounted_sum = 0

            #calculate the discounted sum of rewards
            for step in self.replay_mem[::-1]:
                avg_reward = np.average(step["rewards"])
                avg_discounted_sum = avg_reward + self.gamma * avg_discounted_sum
                discounted_sum = step["rewards"][0] + self.gamma * discounted_sum
                #iterated inversly therefore insert at beginning of array
                c_returns.insert(0, avg_discounted_sum)
                returns.insert(0, discounted_sum)

            #normalise returns
            c_returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)
            
            #backpropagation for critic network
            #only the master agent need calculate the update for the critic net as all updates would be the same
            with tf.GradientTape() as tape:
                self.values = self.critic_net(all_obv_batches, all_action_batches)
                critic_loss = self.loss_fn(self.values, returns)

            c_grads = tape.gradient(critic_loss, self.critic_net.trainable_variables)
            self.c_opt.apply_gradients(zip(c_grads, self.critic_net.trainable_variables))

        else:
            #samples of each piece of data of this agent
            obv_batch = np.array([self.replay_mem[i]["obv"] for i in range(np.shape(self.replay_mem)[0])])
            action_batch = np.array([self.replay_mem[i]["action"] for i in range(np.shape(self.replay_mem)[0])])

            #calculate the discounted sum of rewards
            for step in self.replay_mem[::-1]:
                discounted_sum = step["reward"] + self.gamma * discounted_sum
                #iterated inversly therefore insert at beginning of array
                returns.insert(0, discounted_sum)

        #normalise returns
        returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)

        #backpropagation for actor network
        with tf.GradientTape() as tape:
            #actor networks should be updated using the updated critic net
            action_probs = self.actor_net(obv_batch)

            actor_loss = 0
            for i in range(np.shape(action_probs)[0]):
                #log probability of the action taken
                action_log_prob = tf.math.log(action_probs[i, action_batch[i]])
                advantage = returns[i] - self.values[i]
                #sum losses for both actor and critic across episode
                actor_loss += -action_log_prob * advantage

        a_grads = tape.gradient(actor_loss, self.actor_net.trainable_variables)
        self.a_opt.apply_gradients(zip(a_grads, self.actor_net.trainable_variables))

        #replay memory only stores a single episode 
        self.replay_mem.clear()

        return actor_loss

    def send_comm(self):
        """
            function to send a communication to another agent

            returns a tuple (critic_net_weights, latest_replay_mem_entry) where critic_net_weights are 
            the weights of the critic net if the agent is the master and 0 (a placeholder) otherwise and
            latest_replay_mem_entry is a dict of {"obvs", "actions", "rewards"} containing the data from
            the most recent time step
        """
        if self.master:
            #master sends the values output from the global critic net
            comm = self.values
        else:
            #slave sends the most recent tuple from the replay memory
            comm = self.replay_mem[-1]

        return comm

    def receive_comm(self, comm):
        """
            function to receive a communication from another agent

            comm is either values output from the global critic net or a dict of {"obvs", "actions", "rewards"} 
            containing the data from the most recent time step for that agent
        """
        if self.master:
            #master receives a tuple from another agents replay memory
            #append tuple contents to replay mem in appropriate place 
            self.replay_mem[-1]["obvs"].append(comm["obv"])
            self.replay_mem[-1]["actions"].append(comm["action"])
            self.replay_mem[-1]["rewards"].append(comm["reward"])
        else:
            #slave receives the values output from the global critic net
            self.values = comm

class ActorNet(tf.keras.Model):
    """
        Class to contain the neural network approximating the policy (actor)
    """
    def __init__(self, sizes):
        """
            function to initialise neural network

            sizes is an array of [observations, hidden_size, actions] where observations is an array of 
            [observations_low, observations_high], hidden_size is the number of neurons in the hidden layer 
            and actions is an array of [actions_low, actions_high] in turn low is an array of low bounds for 
            each observation/action and high is an array of high bounds for each observation/action respectively
        """
        super(ActorNet, self).__init__()
        self.hidden1 = tf.keras.layers.Dense(np.shape(sizes[0])[1], activation="relu")
        self.hidden2 = tf.keras.layers.Dense(sizes[1], activation="relu")
        self.actor = tf.keras.layers.Dense(np.shape(sizes[2])[1], activation="softmax")

    def call(self, obv):
        """
            function to define the forward pass of the neural network this function is called 
            when ActorCriticNet(inputs) is called or ActorCriticNet.predict(inputs) is called

            obv is the numpy array or tensor of the inputs values to the neural network

            returns a tensor of the probability distribution of the policy
        """
        obv = self.hidden1(obv)
        obv = self.hidden2(obv)
        policy = self.actor(obv)

        return policy

class CriticNet(tf.keras.Model):
    """
        Class to contain the neural network approximating the Q-value function (critic)
    """
    def __init__(self, sizes, n_agents):
        """
            function to initialise neural network

            sizes is an array of [observations, hidden_size, actions] where observations is an array of 
            [observations_low, observations_high], hidden_size is the number of neurons in the hidden layer 
            and actions is an array of [actions_low, actions_high] in turn low is an array of low bounds for 
            each observation/action and high is an array of high bounds for each observation/action respectively
        """
        super(CriticNet, self).__init__()
        self.obv_hidden1 = tf.keras.layers.Dense((np.shape(sizes[0])[1] * n_agents), activation="relu")
        self.action_hidden1 = tf.keras.layers.Dense((np.shape(sizes[2])[1] * n_agents), activation="relu")

        self.concatenate = tf.keras.layers.Concatenate()
        
        self.hidden2 = tf.keras.layers.Dense(sizes[1], activation="relu")
        self.critic = tf.keras.layers.Dense(1, activation="linear")

    def call(self, obvs, actions):
        """
            function to define the forward pass of the neural network this function is called 
            when CriticNet(inputs) is called or CriticNet.predict(inputs) is called

            obv is the numpy array or tensor of the inputs values to the neural network

            actions is an array of actions taken by all agents

            returns a tensor of the Q-value output by the neural network
        """
        obvs = self.obv_hidden1(obvs)
        actions = self.action_hidden1(actions)

        obvs_actions = self.concatenate([obvs, actions])

        obvs_actions = self.hidden2(obvs_actions)
        value = self.critic(obvs_actions)

        return value



